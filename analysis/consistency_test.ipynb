{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42089e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Define directory where JSON files are stored\n",
    "directory = 'Probing_Uncertainy/mistraljson_oneshot'\n",
    "model_name = 'mistral_hellaswag' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65609c",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfdd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new folder version\n",
    "import pickle\n",
    "methods = ['embedding', 'nota', 'reflect', 'token', 'moreinfo','calibration']\n",
    "variants = ['','_blank_space_0','_blank_space_1', '_blank_space_2',  '_shuffled_option_0','_shuffled_option_1','_shuffled_option_2', '_typo_0',  '_typo_1','_typo_2']\n",
    "for method in methods:\n",
    "# Load the object from the file\n",
    "#{model_name}_mmlu_{variant}\n",
    "    print(\"method\", method)\n",
    "    for variant in variants:\n",
    "    \n",
    "    #for i in range(1,5):\n",
    "        with open(f\"Probing_Uncertainy/pickle_result/{method}_{model_name}{variant}_setup_0_result.pkl\", \"rb\") as file:  # Use \"rb\" mode for reading binary\n",
    "            loaded_data = pickle.load(file)\n",
    "            #print(loaded_data.keys())\n",
    "        #print(\"Method:\", method, \"index\", \"Variant: \" ,variant)\n",
    "\n",
    "        print(loaded_data['abstain_rate'])\n",
    "        #loaded_data['reliable_accuracy'],\",\",loaded_data['effective_reliability'],\",\",loaded_data['abstain_accuracy'],\",\",loaded_data['abstain_precision'],\",\",loaded_data['abstain_recall'],\",\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2730cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "methods = ['nota', 'reflect', 'token', 'moreinfo','calibration']#'embedding', \n",
    "variants = ['_typo_0', '_blank_space_0', '_shuffled_option_0','_typo_1', '_blank_space_1', '_shuffled_option_1','']\n",
    "for method in methods:\n",
    "    print(\"Method:\", method)\n",
    "# Load the object from the file\n",
    "    #for i in range(1,5):\n",
    "    for variant in variants:\n",
    "        with open(f\"Probing_Uncertainy/pickle_result/{method}_mistral_hellaswag{variant}_setup_0_result.pkl\", \"rb\") as file:  # Use \"rb\" mode for reading binary\n",
    "            loaded_data = pickle.load(file)\n",
    "        print(\"Method:\", method, \"Variant: \" ,variant)\n",
    "\n",
    "        print(loaded_data['reliable_accuracy'],\",\",loaded_data['effective_reliability'],\",\",loaded_data['abstain_accuracy'],\",\",loaded_data['abstain_precision'],\",\",loaded_data['abstain_recall'],\",\",loaded_data['abstain_recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce94ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of methods\n",
    "methods = ['embedding', 'nota', 'reflect', 'token', 'moreinfo','calibration']\n",
    "variants = ['typo', 'blank_space', 'shuffled_option','original']\n",
    "# Function to calculate accuracy when abstain is 0 and overall accuracy\n",
    "def calculate_accuracy(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total_count_abstain_0 = 0  # Total number of examples where abstain is 0\n",
    "    correct_count_abstain_0 = 0  # Correct predictions where abstain is 0\n",
    "    total_count_abstain_all = 0  # Total number of examples\n",
    "    correct_count_abstain_all = 0  # Correct predictions in all cases (abstain=0 or 1)\n",
    "\n",
    "    # Iterate over the entries in the JSON file\n",
    "    for entry in data:\n",
    "        decision = entry['decision']\n",
    "        prediction = entry['prediction']\n",
    "        gold_answer = entry['gold_answer']\n",
    "\n",
    "        # Correct prediction is when prediction is 1 (assuming that 1 means correct)\n",
    "        is_correct = 1 if prediction == gold_answer else 0\n",
    "\n",
    "        # Count for all predictions (both abstain=0 and abstain=1)\n",
    "        total_count_abstain_all += 1\n",
    "        if is_correct == 1:\n",
    "            correct_count_abstain_all += 1\n",
    "\n",
    "        # Only count for abstain=0 (i.e., when the model answers)\n",
    "        if decision == 0:  # Abstain flag is 0,\"answer\"\n",
    "            total_count_abstain_0 += 1\n",
    "            if is_correct == 1:\n",
    "                correct_count_abstain_0 += 1\n",
    "\n",
    "    # Calculate accuracy for abstain=0\n",
    "    accuracy_abstain_0 = correct_count_abstain_0 / total_count_abstain_0 if total_count_abstain_0 > 0 else 0\n",
    "\n",
    "    # Calculate overall accuracy (all cases)\n",
    "    accuracy_abstain_all = correct_count_abstain_all / total_count_abstain_all if total_count_abstain_all > 0 else 0\n",
    "\n",
    "    return accuracy_abstain_0, accuracy_abstain_all\n",
    "\n",
    "# Loop over each method and each setup to calculate the accuracy\n",
    "for method in methods:\n",
    "    for variant in variants:\n",
    "        for i in range(1):  # Assuming there are 5 setups (0 to 4)\n",
    "            json_file_path = f'{directory}/{method}_{model_name}_{variant}_setup_{i}.json'\n",
    "\n",
    "            # Calculate accuracy for this setup\n",
    "            accuracy_abstain_0, accuracy_abstain_all = calculate_accuracy(json_file_path)\n",
    "\n",
    "            # Print the results for this setup\n",
    "            print(f'Method: {method}, Variant: {variant}, Setup: {i}')\n",
    "            print(f'  Accuracy (abstain=0): {accuracy_abstain_0:.4f}')\n",
    "            print(f'  Overall Accuracy: {accuracy_abstain_all:.4f}')\n",
    "            print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d7680",
   "metadata": {},
   "source": [
    "# Intra-method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cd869",
   "metadata": {},
   "source": [
    "# Consistency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "# Initialize data containers\n",
    "rejections = defaultdict(set)\n",
    "acceptances = defaultdict(set)\n",
    "predictions = defaultdict(lambda: defaultdict(int))\n",
    "total_questions_per_setup = defaultdict(int)\n",
    "\n",
    "# Define utility functions\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def harmonic_mean(iou1, iou2):\n",
    "    return (2 * iou1 * iou2) / (iou1 + iou2) if (iou1 + iou2) > 0 else 0\n",
    "\n",
    "def calculate_agreement(set1, set2, predictions1, predictions2):\n",
    "    common_questions = set1.intersection(set2)\n",
    "    same_predictions = sum(\n",
    "        predictions1[q] == predictions2[q] for q in common_questions\n",
    "    )\n",
    "    total_questions = len(common_questions)\n",
    "    return same_predictions / total_questions if total_questions > 0 else 0\n",
    "\n",
    "def calculate_common_accept_accuracy(set1, set2, predictions1, predictions2):\n",
    "    common_questions = set1.intersection(set2)\n",
    "    correct_predictions = sum(\n",
    "        (predictions1[q] + predictions2[q]) / 2 for q in common_questions\n",
    "    )\n",
    "    total_questions = len(common_questions)\n",
    "    return correct_predictions / total_questions if total_questions > 0 else 0\n",
    "\n",
    "def calculate_rejection_rate(rejections, total_questions):\n",
    "    return len(rejections) / total_questions if total_questions > 0 else 0\n",
    "\n",
    "# JSON file processor\n",
    "def process_json_file(file_path, eval_index):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        total_questions = len(data)\n",
    "        total_questions_per_setup[eval_index] = total_questions\n",
    "\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']\n",
    "            prediction = entry['prediction']\n",
    "            gold_answer = entry['gold_answer']\n",
    "\n",
    "            if decision == 1:\n",
    "                rejections[eval_index].add(question_idx)\n",
    "            elif decision == 0:\n",
    "                acceptances[eval_index].add(question_idx)\n",
    "                predictions[eval_index][question_idx] = int(prediction == gold_answer)\n",
    "\n",
    "# Process method and variant groups\n",
    "def process_json_files_for_method_variant(directory, model_name, method, variant_folders, group_name):\n",
    "    # Clear previous data\n",
    "    rejections.clear()\n",
    "    acceptances.clear()\n",
    "    predictions.clear()\n",
    "    total_questions_per_setup.clear()\n",
    "\n",
    "    # Read files\n",
    "    for variant_folder in variant_folders:\n",
    "        variant_path = os.path.join(directory, variant_folder)\n",
    "        if not os.path.isdir(variant_path):\n",
    "            print(f\"Warning: Directory {variant_path} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(variant_path):\n",
    "            if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "                file_path = os.path.join(variant_path, filename)\n",
    "                match = re.search(r\"mmlu_(.*?)_setup\", filename)\n",
    "                if match:\n",
    "                    eval_index = match.group(1)\n",
    "                process_json_file(file_path, eval_index)\n",
    "\n",
    "    # Calculate metrics\n",
    "    setup_indices = list(total_questions_per_setup.keys())\n",
    "    harmonic_means, agreements, common_accept_accuracies = [], [], []\n",
    "    rejection_ious, acceptance_ious = [], []\n",
    "    rejection_rates = []\n",
    "\n",
    "    for i, j in combinations(setup_indices, 2):\n",
    "        if i in rejections and j in rejections and i in acceptances and j in acceptances and i[1] == j[1]:\n",
    "            rejection_iou = calculate_iou(rejections[i], rejections[j])\n",
    "            rejection_ious.append(rejection_iou)\n",
    "\n",
    "            acceptance_iou = calculate_iou(acceptances[i], acceptances[j])\n",
    "            acceptance_ious.append(acceptance_iou)\n",
    "            print(\"acceptance\", i, j, acceptance_iou)\n",
    "\n",
    "            hm = harmonic_mean(rejection_iou, acceptance_iou)\n",
    "            harmonic_means.append(hm)\n",
    "            print(i,j,hm)\n",
    "\n",
    "            agreement = calculate_agreement(acceptances[i], acceptances[j], predictions[i], predictions[j])\n",
    "            agreements.append(agreement)\n",
    "            print(\"agreement\", i, j, agreement)\n",
    "\n",
    "            common_accept_accuracy = calculate_common_accept_accuracy(acceptances[i], acceptances[j], predictions[i], predictions[j])\n",
    "            common_accept_accuracies.append(common_accept_accuracy)\n",
    "\n",
    "    for idx in setup_indices:\n",
    "        rejection_rate = calculate_rejection_rate(rejections[idx], total_questions_per_setup[idx])\n",
    "        rejection_rates.append(rejection_rate)\n",
    "\n",
    "    # Calculate and display mean and stddev\n",
    "    metrics = {\n",
    "        \"Harmonic Mean\": harmonic_means,\n",
    "        \"Rejection IOU\": rejection_ious,\n",
    "        \"Acceptance IOU\": acceptance_ious,\n",
    "        \"Agreement\": agreements,\n",
    "        \"Common Accept Accuracy\": common_accept_accuracies,\n",
    "        \"Rejection Rate\": rejection_rates,\n",
    "    }\n",
    "\n",
    "    print(f\"Results for group '{group_name}' and method '{method}':\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        mean = sum(values) / len(values) if values else 0\n",
    "        stddev = statistics.stdev(values) if len(values) > 1 else 0\n",
    "        print(f\"{metric_name}: Mean = {mean:.3f}, StdDev = {stddev:.3f}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "# Main processing loop\n",
    "methods = ['embedding', 'nota', 'reflect', 'token', 'moreinfo', 'calibration']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "variant_groups = {\n",
    "    \"typo\": [f\"{dataset}_typo_1\", f\"{dataset}_typo_2\",f\"{dataset}_typo_0\"],\n",
    "    \"blank_space\": [f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\",f\"{dataset}_blank_space_0\"],\n",
    "    \"shuffled_option\": [f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\",f\"{dataset}_shuffled_option_0\"],\n",
    "}\n",
    "\n",
    "for method in methods:\n",
    "    for group_name, variant_folders in variant_groups.items():\n",
    "        process_json_files_for_method_variant(directory, model_name, method, variant_folders, group_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_files_for_method_variant(directory, model_name, method, variant_folders, group_name):\n",
    "    # Clear previous data\n",
    "    rejections.clear()\n",
    "    acceptances.clear()\n",
    "    predictions.clear()\n",
    "    total_questions_per_setup.clear()\n",
    "\n",
    "    # Read files\n",
    "    for variant_folder in variant_folders:\n",
    "        variant_path = os.path.join(directory, variant_folder)\n",
    "        if not os.path.isdir(variant_path):\n",
    "            print(f\"Warning: Directory {variant_path} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(variant_path):\n",
    "            if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "                file_path = os.path.join(variant_path, filename)\n",
    "                print(file_path)\n",
    "                match = re.search(r\"hellaswag_(.*?)_setup\", filename)\n",
    "                if match:\n",
    "                    eval_index = match.group(1)\n",
    "                    process_json_file(file_path, eval_index)\n",
    "\n",
    "    # Ensure only same-variable-group comparisons\n",
    "    setup_indices = list(total_questions_per_setup.keys())\n",
    "    harmonic_means, agreements, common_accept_accuracies = [], [], []\n",
    "    rejection_ious, acceptance_ious = [], []\n",
    "    rejection_rates = []\n",
    "\n",
    "    for i, j in combinations(setup_indices, 2):\n",
    "        #print(i,j)\n",
    "        # Skip comparisons if indices don't match the group\n",
    "        if i.split(\"_\")[0] != j.split(\"_\")[0]:  # Ensure same variable group (e.g., typo_1, typo_2)\n",
    "            continue\n",
    "\n",
    "        if i in rejections and j in rejections and i in acceptances and j in acceptances:\n",
    "            rejection_iou = calculate_iou(rejections[i], rejections[j])\n",
    "            rejection_ious.append(rejection_iou)\n",
    "\n",
    "            acceptance_iou = calculate_iou(acceptances[i], acceptances[j])\n",
    "            acceptance_ious.append(acceptance_iou)\n",
    "\n",
    "            hm = harmonic_mean(rejection_iou, acceptance_iou)\n",
    "            harmonic_means.append(hm)\n",
    "\n",
    "            agreement = calculate_agreement(acceptances[i], acceptances[j], predictions[i], predictions[j])\n",
    "            agreements.append(agreement)\n",
    "\n",
    "            common_accept_accuracy = calculate_common_accept_accuracy(acceptances[i], acceptances[j], predictions[i], predictions[j])\n",
    "            common_accept_accuracies.append(common_accept_accuracy)\n",
    "\n",
    "    for idx in setup_indices:\n",
    "        rejection_rate = calculate_rejection_rate(rejections[idx], total_questions_per_setup[idx])\n",
    "        rejection_rates.append(rejection_rate)\n",
    "\n",
    "    # Calculate and display mean and stddev\n",
    "    metrics = {\n",
    "        \"Harmonic Mean\": harmonic_means,\n",
    "        \"Acceptance IOU\": acceptance_ious,\n",
    "        \"Rejection IOU\": rejection_ious,\n",
    "        \"Common Accept Accuracy\": common_accept_accuracies,\n",
    "        \"Agreement\": agreements,\n",
    "        \"Rejection Rate\": rejection_rates,\n",
    "    }\n",
    "\n",
    "    #print(f\"Results for method '{method}' and group '{group_name}':\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        mean = sum(values) / len(values) if values else 0\n",
    "        stddev = statistics.stdev(values) if len(values) > 1 else 0\n",
    "        print(f\"{mean:.3f} Â± {stddev:.3f}\" , end = \",\")\n",
    "    print('')\n",
    "\n",
    "# Main processing loop\n",
    "methods = ['embedding', 'nota', 'reflect', 'token', 'moreinfo', 'calibration']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_hellaswag\"\n",
    "variant_groups = {\n",
    "    \"blank_space\": [f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\",f\"{dataset}_blank_space_0\"],#,\"mistraljson_variant\"],\n",
    "    \"shuffled_option\": [f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\",f\"{dataset}_shuffled_option_0\"],#\"mistraljson_variant\"]\n",
    "\n",
    "    \"typo\": [f\"{dataset}_typo_1\", f\"{dataset}_typo_2\",f\"{dataset}_typo_0\"]#\"mistraljson_variant\"],\n",
    "    }\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"Processing method: {method}\")\n",
    "    for group_name, variant_folders in variant_groups.items():\n",
    "        process_json_files_for_method_variant(directory, model_name, method, variant_folders, group_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "rejections = defaultdict(set)\n",
    "acceptances = defaultdict(set)\n",
    "predictions_raw = defaultdict(lambda: defaultdict(int))  \n",
    "predictions_binary = defaultdict(lambda: defaultdict(int))  \n",
    "total_questions_per_setup = defaultdict(int)\n",
    "\n",
    "\n",
    "def process_json_file(file_path, eval_index):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        total_questions = len(data)\n",
    "        total_questions_per_setup[eval_index] = total_questions\n",
    "\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']\n",
    "            prediction = entry['prediction']  \n",
    "            gold_answer = entry['gold_answer']\n",
    "\n",
    "            if decision == 1:\n",
    "                rejections[eval_index].add(question_idx)\n",
    "            elif decision == 0:\n",
    "                acceptances[eval_index].add(question_idx)\n",
    "                predictions_raw[eval_index][question_idx] = prediction  \n",
    "                predictions_binary[eval_index][question_idx] = int(prediction == gold_answer)  \n",
    "\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def harmonic_mean(a, b):\n",
    "    return 2 * a * b / (a + b) if (a + b) > 0 else 0\n",
    "\n",
    "def calculate_rejection_rate(rejected_set, total_count):\n",
    "    return len(rejected_set) / total_count if total_count > 0 else 0\n",
    "\n",
    "def calculate_agreement(set1, set2, predictions1, predictions2, idx):\n",
    "    common_questions = set1.intersection(set2)\n",
    "\n",
    "    if \"shuffled_option\" in idx:\n",
    "        with open(f\"Probing_Uncertainy/data/mmlu_{idx}.json\", 'r') as file:\n",
    "            data = json.load(file)\n",
    "        with open(f\"Probing_Uncertainy/data/mmlu.json\", 'r') as file:\n",
    "            data_original = json.load(file)\n",
    "\n",
    "        same_predictions = 0\n",
    "        \n",
    "        for q in common_questions:\n",
    "            if predictions1[q]!= 'E' and predictions1[q]!= 'Z' and predictions2[q]!= 'E' and predictions2[q]!= 'Z':\n",
    "                choice1 = data_original['test'][q]['choices'][predictions1[q]]\n",
    "                choice2 = data['test'][q]['choices'][predictions2[q]]\n",
    "                same_predictions += (choice1 == choice2)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "                \n",
    "    else:\n",
    "        same_predictions = sum(\n",
    "            predictions1[q] == predictions2[q]\n",
    "            for q in common_questions if q in predictions1 and q in predictions2\n",
    "        )\n",
    "\n",
    "    total_questions = len(common_questions)\n",
    "    return same_predictions / total_questions if total_questions > 0 else 0\n",
    "\n",
    "def calculate_common_accept_accuracy(set1, set2, predictions1, predictions2):\n",
    "    common_accept = set1.intersection(set2)\n",
    "    correct_predictions = sum(\n",
    "        predictions1[q] == predictions2[q]\n",
    "        for q in common_accept if q in predictions1 and q in predictions2\n",
    "    )\n",
    "    total_common_accept = len(common_accept)\n",
    "    return correct_predictions / total_common_accept if total_common_accept > 0 else 0\n",
    "\n",
    "def calculate_common_flag_ratio(accept1, reject1, accept2, reject2):\n",
    "    \"\"\"Common Flag Ratio\"\"\"\n",
    "    common_accept = accept1 & accept2\n",
    "    common_reject = reject1 & reject2\n",
    "    numerator = len(common_accept) + len(common_reject)\n",
    "    \n",
    "    all_questions = accept1.union(accept2).union(reject1).union(reject2)\n",
    "    denominator = len(all_questions)\n",
    "    \n",
    "    return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "def process_json_files_with_original(directory, model_name, method, variant_folders, original_folder, group_name):\n",
    "    rejections.clear()\n",
    "    acceptances.clear()\n",
    "    predictions_raw.clear()\n",
    "    predictions_binary.clear()\n",
    "    total_questions_per_setup.clear()\n",
    "\n",
    "    original_path = os.path.join(directory, original_folder)\n",
    "    if not os.path.isdir(original_path):\n",
    "        print(f\"Error: Directory {original_path} does not exist. Unable to compare with original.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(original_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(original_path, filename)\n",
    "            match = re.search(f\"{method}_{model_name}_mmlu_original_setup_0.json\", filename)\n",
    "            if match:\n",
    "                eval_index = \"original\"\n",
    "                process_json_file(file_path, eval_index)\n",
    "\n",
    "    original_rejections = rejections.copy()\n",
    "    original_acceptances = acceptances.copy()\n",
    "    original_predictions_raw = predictions_raw.copy()\n",
    "    original_predictions_binary = predictions_binary.copy()\n",
    "\n",
    "    for variant_folder in variant_folders:\n",
    "        variant_path = os.path.join(directory, variant_folder)\n",
    "        if not os.path.isdir(variant_path):\n",
    "            print(f\"Warning: Directory {variant_path} does not exist. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(variant_path):\n",
    "            if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "                file_path = os.path.join(variant_path, filename)\n",
    "                match = re.search(r\"mmlu_(.*?)\", filename)#hellaswag_(.*?)hellaswag_(.*?)setup_([1-9]\\d*)\n",
    "                if match:\n",
    "                    eval_index = match.group(1)\n",
    "                    process_json_file(file_path, eval_index)\n",
    "                print(file_path)\n",
    "\n",
    "    variant_indices = list(total_questions_per_setup.keys())\n",
    "    comparison_metrics = {\n",
    "        \"Variant\": [],\n",
    "        \"Harmonic Mean\": [],\n",
    "        \"Acceptance IOU\": [],\n",
    "        \"Rejection IOU\": [],\n",
    "        \"Common Accept Accuracy\": [],\n",
    "        \"Agreement\": [],\n",
    "        \"Common Flag Ratio\": [], \n",
    "        \"Rejection Rate\": []\n",
    "    }\n",
    "\n",
    "    for variant_folder in variant_folders:\n",
    "        variant_path = os.path.join(directory, variant_folder)\n",
    "\n",
    "        rejections.clear()\n",
    "        acceptances.clear()\n",
    "        predictions_raw.clear()\n",
    "        predictions_binary.clear()\n",
    "        total_questions_per_setup.clear()\n",
    "\n",
    "        for filename in os.listdir(variant_path):\n",
    "            if model_name in filename and method in filename and group_name in filename and filename.endswith('.json'):\n",
    "                file_path = os.path.join(variant_path, filename)\n",
    "                match = re.search(r\"mmlu_(.*?)\", filename)#hellaswag_setup_(\\d+)hellaswag_(.*?)mmlu_setup_(\\d+)\n",
    "                if match:\n",
    "                    eval_index = match.group(1)\n",
    "                    process_json_file(file_path, eval_index)\n",
    "\n",
    "        for idx in variant_indices:\n",
    "            rejection_iou = calculate_iou(original_rejections[\"original\"], rejections[idx])\n",
    "            acceptance_iou = calculate_iou(original_acceptances[\"original\"], acceptances[idx])\n",
    "            hm = harmonic_mean(rejection_iou, acceptance_iou)\n",
    "\n",
    "            agreement = calculate_agreement(\n",
    "                original_acceptances[\"original\"], acceptances[idx], \n",
    "                original_predictions_raw[\"original\"], predictions_raw[idx], idx\n",
    "            )\n",
    "\n",
    "            common_accept_accuracy = calculate_common_accept_accuracy(\n",
    "                original_acceptances[\"original\"], acceptances[idx], \n",
    "                original_predictions_binary[\"original\"], predictions_binary[idx]\n",
    "            )\n",
    "\n",
    "            # Common Flag Ratio\n",
    "            common_flag_ratio = calculate_common_flag_ratio(\n",
    "                original_acceptances[\"original\"], original_rejections[\"original\"],\n",
    "                acceptances[idx], rejections[idx]\n",
    "            )\n",
    "\n",
    "            rejection_rate = calculate_rejection_rate(rejections[idx], total_questions_per_setup[idx])\n",
    "\n",
    "            if idx != \"original\": #and agreement != 0\n",
    "                print(idx)\n",
    "                print(\"[\", hm, \",\", acceptance_iou, \",\", rejection_iou, \",\", common_accept_accuracy, \",\", agreement, \",\", common_flag_ratio, \"],\")\n",
    "                result.append([ common_flag_ratio])  \n",
    "        comparison_metrics[\"Variant\"].append(variant_folder)\n",
    "\n",
    "methods = ['calibration', 'embedding', 'nota', 'moreinfo', 'reflect', 'token']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_mmlu\"\n",
    "original_folder = \"mistraljson_oneshot\"\n",
    "variant_groups = {\n",
    "    #\"llama3\": [\"llama3_hmmlu\"],\n",
    "\n",
    "    \"blank_space\": [f\"mistraljson_variant\", f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    \"shuffled_option\": [f\"mistraljson_variant\", f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    \"typo\": [f\"mistraljson_variant\", f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"]\n",
    "}\n",
    "result = []\n",
    "for method in methods:\n",
    "    for group_name, variant_folders in variant_groups.items():\n",
    "        process_json_files_with_original(directory, model_name, method, variant_folders, original_folder, group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a021118",
   "metadata": {},
   "source": [
    "# Cross-method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68190e51",
   "metadata": {},
   "source": [
    "## Heatmap seprately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e854140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate the ratio of common flags (both abstain or both answer)\n",
    "def calculate_common_flag_ratio(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "    \n",
    "    total_questions = len(flags1)\n",
    "    same_flags_count = sum(1 for q in flags1 if flags1[q] == flags2.get(q, None))\n",
    "\n",
    "    return same_flags_count / total_questions if total_questions > 0 else 0\n",
    "\n",
    "# Function to process JSON files for a specific method and subdirectory\n",
    "def process_json_files_for_method(directory, subdirectory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "    subdirectory_path = os.path.join(directory, subdirectory)\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(subdirectory_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(subdirectory_path, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "            print(file_path)\n",
    "\n",
    "    return flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# List of methods to process\n",
    "methods = ['calibration', 'embedding', 'nota', 'moreinfo', 'token', 'reflect']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_mmlu\"\n",
    "variant_groups = {\n",
    "    \"original\": [f\"mistraljson_variant\"],\n",
    "    \"blank_space\": [f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    \"shuffled_option\": [f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    \"typo\": [f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"],\n",
    "}\n",
    "\n",
    "# Process each variant group\n",
    "for group_name, subdirectories in variant_groups.items():\n",
    "    print(f\"Processing variant group: {group_name}\")\n",
    "    \n",
    "    # Initialize matrix to accumulate values across setups\n",
    "    n_methods = len(methods)\n",
    "    avg_matrix_per_setup = np.zeros((n_methods, n_methods, 5))  # 5 setups, n_methods x n_methods for each setup\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        flags_per_method = defaultdict(dict)  # Store flags per method for each subdirectory\n",
    "\n",
    "        # Process all methods for the current subdirectory\n",
    "        for method in methods:\n",
    "            flags_per_method[method] = process_json_files_for_method(directory, subdirectory, model_name, method)\n",
    "\n",
    "        # Compute pairwise common flag ratios for all setups\n",
    "        for setup_index in range(1):  # Setup indices from 0 to 4\n",
    "            for i in range(n_methods):\n",
    "                method_i = methods[i]\n",
    "                flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "                \n",
    "                for j in range(i, n_methods):\n",
    "                    method_j = methods[j]\n",
    "                    flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "                    \n",
    "                    # Calculate the common flag ratio\n",
    "                    if i == j:\n",
    "                        common_flag_ratio = 1  # Diagonal value\n",
    "                    else:\n",
    "                        common_flag_ratio = calculate_common_flag_ratio(flags_i, flags_j)\n",
    "                    \n",
    "                    # Accumulate values\n",
    "                    avg_matrix_per_setup[i, j, setup_index] += common_flag_ratio\n",
    "                    if i != j:\n",
    "                        avg_matrix_per_setup[j, i, setup_index] += common_flag_ratio  # Symmetric matrix\n",
    "\n",
    "    # Average across subdirectories\n",
    "    avg_matrix_per_setup /= len(subdirectories)\n",
    "\n",
    "    # Plot the heatmap for each setup\n",
    "    for setup_index in range(1):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "        plt.title(f\"{group_name.capitalize()} - Decision Consistency Heatmap (Setup {setup_index})\")\n",
    "        plt.xlabel(\"Methods\")\n",
    "        plt.ylabel(\"Methods\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Define function to calculate Harmonic Mean\n",
    "def harmonic_mean(iou1, iou2):\n",
    "    return (2 * iou1 * iou2) / (iou1 + iou2) if (iou1 + iou2) > 0 else 0\n",
    "\n",
    "# Function to calculate the ratio of common flags (both abstain or both answer)\n",
    "def calculate_common_flag_ratio(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "    \n",
    "    total_questions = len(flags1)\n",
    "    same_flags_count = sum(1 for q in flags1 if flags1[q] == flags2.get(q, None))\n",
    "\n",
    "    return same_flags_count / total_questions if total_questions > 0 else 0\n",
    "\n",
    "# Function to process JSON files for a specific method and subdirectory\n",
    "def process_json_files_for_method(directory, subdirectory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "    subdirectory_path = os.path.join(directory, subdirectory)\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(subdirectory_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json') and \"setup_0\" in filename:\n",
    "            file_path = os.path.join(subdirectory_path, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "            print(file_path)\n",
    "\n",
    "    return flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# List of methods to process\n",
    "methods = ['token', 'calibration', 'embedding', 'nota', 'moreinfo', 'reflect']\n",
    "display_methods = ['TokProb', 'AskCal', 'Embedding', 'NOTA', 'MoreInfo', 'SelfRef']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_mmlu\"\n",
    "variant_groups = {\n",
    "    \"original\": [f\"mistraljson_variant\"],\n",
    "    \"blank_space\": [f\"mistraljson_variant\", f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    \"shuffled_option\": [f\"mistraljson_variant\", f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    \"typo\": [f\"mistraljson_variant\", f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"],\n",
    "}\n",
    "\n",
    "# Process each variant group\n",
    "for group_name, subdirectories in variant_groups.items():\n",
    "    print(f\"Processing variant group: {group_name}\")\n",
    "    \n",
    "    # Initialize matrix to accumulate values across setups\n",
    "    n_methods = len(methods)\n",
    "    avg_matrix_per_setup = np.zeros((n_methods, n_methods, 5))  # 5 setups, n_methods x n_methods for each setup\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        flags_per_method = defaultdict(dict)  # Store flags per method for each subdirectory\n",
    "\n",
    "        # Process all methods for the current subdirectory\n",
    "        for method in methods:\n",
    "            flags_per_method[method] = process_json_files_for_method(directory, subdirectory, model_name, method)\n",
    "\n",
    "        # Compute pairwise common flag ratios for all setups\n",
    "        for setup_index in range(1):  # Setup indices from 0 to 4\n",
    "            for i in range(n_methods):\n",
    "                method_i = methods[i]\n",
    "                flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "                \n",
    "                for j in range(i, n_methods):\n",
    "                    method_j = methods[j]\n",
    "                    flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "                    \n",
    "                    # Calculate the common flag ratio\n",
    "                    if i == j:\n",
    "                        common_flag_ratio = 1  # Diagonal value\n",
    "                    else:\n",
    "                        common_flag_ratio = calculate_common_flag_ratio(flags_i, flags_j)\n",
    "\n",
    "                    #print(i,j,common_flag_ratio)\n",
    "                    \n",
    "                    # Accumulate values\n",
    "                    avg_matrix_per_setup[i, j, setup_index] += common_flag_ratio\n",
    "                    if i != j:\n",
    "                        avg_matrix_per_setup[j, i, setup_index] += common_flag_ratio  # Symmetric matrix\n",
    "\n",
    "    # Average across subdirectories\n",
    "    avg_matrix_per_setup /= len(subdirectories)\n",
    "\n",
    "    # Plot the heatmap for each setup\n",
    "    for setup_index in range(1):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if \"llama\" in dataset:\n",
    "            #sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" LLaMA3, MMLU - Decision Consistency Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"llama_mmlu{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" LLaMA3, Hellaswag - Decision Consistency Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"llama_hellaswag{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        else:\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" Mistral, MMLU - Decision Consistency Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"mistral_mmlu{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" Mistral, Hellaswag - Decision Consistency Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"mistral_hellaswag{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        plt.xlabel(\"Methods\")\n",
    "        plt.ylabel(\"Methods\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18da5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oneshot\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Define function to calculate Harmonic Mean\n",
    "def harmonic_mean(iou1, iou2):\n",
    "    return (2 * iou1 * iou2) / (iou1 + iou2) if (iou1 + iou2) > 0 else 0\n",
    "\n",
    "# Function to calculate the ratio of common flags (both abstain or both answer)\n",
    "def calculate_common_flag_ratio(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "    \n",
    "    total_questions = len(flags1)\n",
    "    same_flags_count = sum(1 for q in flags1 if flags1[q] == flags2.get(q, None))\n",
    "\n",
    "    return same_flags_count / total_questions if total_questions > 0 else 0\n",
    "\n",
    "# Function to process JSON files for a specific method and subdirectory\n",
    "def process_json_files_for_method(directory, subdirectory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "    subdirectory_path = os.path.join(directory, subdirectory)\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(subdirectory_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json'):# and \"setup_0\" in filename\n",
    "            file_path = os.path.join(subdirectory_path, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "            print(file_path)\n",
    "\n",
    "    return flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# List of methods to process\n",
    "methods = ['token', 'calibration', 'embedding', 'nota', 'moreinfo', 'reflect']\n",
    "display_methods = ['TokProb', 'AskCal', 'Embedding', 'NOTA', 'MoreInfo', 'SelfRef']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistraljson_mmlu\"\n",
    "variant_groups = {\n",
    "    # \"original\": [f\"llama3_hellaswag\"],\n",
    "    # \"blank_space\": [f\"{dataset}_blank_space_0\", f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    # \"shuffled_option\": [f\"{dataset}_blank_space_0\", f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    # \"typo\": [f\"{dataset}_blank_space_0\", f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"],\n",
    "    f\"{model_name}\": [\"mistraljson_oneshot\"]\n",
    "}\n",
    "\n",
    "# Process each variant group\n",
    "for group_name, subdirectories in variant_groups.items():\n",
    "    print(f\"Processing variant group: {group_name}\")\n",
    "    \n",
    "    # Initialize matrix to accumulate values across setups\n",
    "    n_methods = len(methods)\n",
    "    avg_matrix_per_setup = np.zeros((n_methods, n_methods, 5))  # 5 setups, n_methods x n_methods for each setup\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        flags_per_method = defaultdict(dict)  # Store flags per method for each subdirectory\n",
    "\n",
    "        # Process all methods for the current subdirectory\n",
    "        for method in methods:\n",
    "            flags_per_method[method] = process_json_files_for_method(directory, subdirectory, model_name, method)\n",
    "\n",
    "        # Compute pairwise common flag ratios for all setups\n",
    "        for setup_index in range(1):  # Setup indices from 0 to 4\n",
    "            for i in range(n_methods):\n",
    "                method_i = methods[i]\n",
    "                flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "                \n",
    "                for j in range(i, n_methods):\n",
    "                    method_j = methods[j]\n",
    "                    flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "                    \n",
    "                    # Calculate the common flag ratio\n",
    "                    if i == j:\n",
    "                        common_flag_ratio = 1  # Diagonal value\n",
    "                    else:\n",
    "                        common_flag_ratio = calculate_common_flag_ratio(flags_i, flags_j)\n",
    "\n",
    "                    #print(i,j,common_flag_ratio)\n",
    "                    \n",
    "                    # Accumulate values\n",
    "                    avg_matrix_per_setup[i, j, setup_index] += common_flag_ratio\n",
    "                    if i != j:\n",
    "                        avg_matrix_per_setup[j, i, setup_index] += common_flag_ratio  # Symmetric matrix\n",
    "\n",
    "    # Average across subdirectories\n",
    "    avg_matrix_per_setup /= len(subdirectories)\n",
    "\n",
    "    # Plot the heatmap for each setup\n",
    "    for setup_index in range(1):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if \"llama\" in dataset:\n",
    "            #sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" LLaMA3, MMLU - Decision Consistency Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"llama_mmlu_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\",  vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" LLaMA3, Hellaswag - Decision Consistency Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"llama_hellaswag_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        else:\n",
    "            if \"json\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\",  vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" Mistral, MMLU - Decision Consistency Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"mistral_mmlu_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "                print(\"i\")\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\",  vmin=0, vmax=1,cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" Mistral, Hellaswag - Decision Consistency Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"mistral_hellaswag_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        plt.xlabel(\"Methods\")\n",
    "        plt.ylabel(\"Methods\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#harmonic heatmap\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate IOU\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Function to calculate Harmonic Mean of IOUs\n",
    "def harmonic_mean(iou1, iou2):\n",
    "    return (2 * iou1 * iou2) / (iou1 + iou2) if (iou1 + iou2) > 0 else 0\n",
    "\n",
    "# Function to calculate the Harmonic IOU\n",
    "def calculate_harmonic_iou(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "\n",
    "    set1_accept = {q for q, d in flags1.items() if d == 0}  # Accepted questions in flags1\n",
    "    set1_reject = {q for q, d in flags1.items() if d == 1}  # Rejected questions in flags1\n",
    "    set2_accept = {q for q, d in flags2.items() if d == 0}  # Accepted questions in flags2\n",
    "    set2_reject = {q for q, d in flags2.items() if d == 1}  # Rejected questions in flags2\n",
    "\n",
    "    # Calculate IOUs for accept and reject sets\n",
    "    iou_accept = calculate_iou(set1_accept, set2_accept)\n",
    "    iou_reject = calculate_iou(set1_reject, set2_reject)\n",
    "\n",
    "    # Calculate Harmonic Mean of IOUs\n",
    "    return harmonic_mean(iou_accept, iou_reject)\n",
    "\n",
    "# Function to process JSON files for a specific method and subdirectory\n",
    "def process_json_files_for_method(directory, subdirectory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "    subdirectory_path = os.path.join(directory, subdirectory)\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(subdirectory_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json') and \"setup_0\" in filename:\n",
    "            file_path = os.path.join(subdirectory_path, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "\n",
    "    return flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# Main script parameters\n",
    "methods = ['token', 'calibration', 'embedding', 'nota', 'moreinfo', 'reflect']\n",
    "display_methods = ['TokProb', 'AskCal', 'Embedding', 'NOTA', 'MoreInfo', 'SelfRef']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_mmlu\"\n",
    "variant_groups = {\n",
    "    \"original\": [f\"mistraljson_variant\"],\n",
    "    \"blank_space\": [f\"mistraljson_variant\", f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    \"shuffled_option\": [f\"mistraljson_variant\", f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    \"typo\": [f\"mistraljson_variant\", f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"],\n",
    "}\n",
    "\n",
    "# Process each variant group\n",
    "for group_name, subdirectories in variant_groups.items():\n",
    "    print(f\"Processing variant group: {group_name}\")\n",
    "    \n",
    "    # Initialize matrix to accumulate values across setups\n",
    "    n_methods = len(methods)\n",
    "    avg_matrix_per_setup = np.zeros((n_methods, n_methods, 5))  # 5 setups, n_methods x n_methods for each setup\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        flags_per_method = defaultdict(dict)  # Store flags per method for each subdirectory\n",
    "\n",
    "        # Process all methods for the current subdirectory\n",
    "        for method in methods:\n",
    "            flags_per_method[method] = process_json_files_for_method(directory, subdirectory, model_name, method)\n",
    "\n",
    "        # Compute pairwise Harmonic IOU for all setups\n",
    "        for setup_index in range(5):  # Setup indices from 0 to 4\n",
    "            for i in range(n_methods):\n",
    "                method_i = methods[i]\n",
    "                flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "                \n",
    "                for j in range(i, n_methods):\n",
    "                    method_j = methods[j]\n",
    "                    flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "                    \n",
    "                    # Calculate the Harmonic IOU\n",
    "                    if i == j:\n",
    "                        harmonic_iou = 1  # Diagonal value\n",
    "                    else:\n",
    "                        harmonic_iou = calculate_harmonic_iou(flags_i, flags_j)\n",
    "\n",
    "                    # Accumulate values\n",
    "                    avg_matrix_per_setup[i, j, setup_index] += harmonic_iou\n",
    "                    if i != j:\n",
    "                        avg_matrix_per_setup[j, i, setup_index] += harmonic_iou  # Symmetric matrix\n",
    "\n",
    "    # Average across subdirectories\n",
    "    avg_matrix_per_setup /= len(subdirectories)\n",
    "\n",
    "    # Plot the heatmap for each setup\n",
    "    for setup_index in range(1):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if \"llama\" in dataset:\n",
    "        #sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" LLaMA3, MMLU - Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"h_llama_mmlu{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" LLaMA3, Hellaswag - Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"h_llama_hellaswag{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        else:\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\", vmin=0, vmax=1,  cbar_kws={'ticks': [1.0, 0.8, 0.6, 0.4, 0.2]})\n",
    "                plt.title(f\" Mistral, MMLU - Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"h_mistral_mmlu{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" Mistral, Hellaswag - Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "                plt.savefig(f\"h_mistral_hellaswag{group_name.capitalize()}.pdf\", format='pdf', bbox_inches='tight')\n",
    "        # sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "        # plt.title(f\"Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "        # plt.xlabel(\"Methods\")\n",
    "        # plt.ylabel(\"Methods\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f\"{model_name}_{group_name}_harmonic_iou.pdf\", format='pdf', bbox_inches='tight')\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#harmonic shot\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate IOU\n",
    "def calculate_iou(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Function to calculate Harmonic Mean of IOUs\n",
    "def harmonic_mean(iou1, iou2):\n",
    "    return (2 * iou1 * iou2) / (iou1 + iou2) if (iou1 + iou2) > 0 else 0\n",
    "\n",
    "# Function to calculate the Harmonic IOU\n",
    "def calculate_harmonic_iou(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "\n",
    "    set1_accept = {q for q, d in flags1.items() if d == 0}  # Accepted questions in flags1\n",
    "    set1_reject = {q for q, d in flags1.items() if d == 1}  # Rejected questions in flags1\n",
    "    set2_accept = {q for q, d in flags2.items() if d == 0}  # Accepted questions in flags2\n",
    "    set2_reject = {q for q, d in flags2.items() if d == 1}  # Rejected questions in flags2\n",
    "\n",
    "    # Calculate IOUs for accept and reject sets\n",
    "    iou_accept = calculate_iou(set1_accept, set2_accept)\n",
    "    iou_reject = calculate_iou(set1_reject, set2_reject)\n",
    "\n",
    "    # Calculate Harmonic Mean of IOUs\n",
    "    return harmonic_mean(iou_accept, iou_reject)\n",
    "\n",
    "# Function to process JSON files for a specific method and subdirectory\n",
    "def process_json_files_for_method(directory, subdirectory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "    subdirectory_path = os.path.join(directory, subdirectory)\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(subdirectory_path):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json') :#and \"setup_0\" in filename\n",
    "            file_path = os.path.join(subdirectory_path, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "            print(file_path)\n",
    "\n",
    "    return flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# Main script parameters\n",
    "methods = ['token', 'calibration', 'embedding', 'nota', 'moreinfo', 'reflect']\n",
    "display_methods = ['TokProb', 'AskCal', 'Embedding', 'NOTA', 'MoreInfo', 'SelfRef']\n",
    "model_name = \"mistral\"\n",
    "directory = \"Probing_Uncertainy\"\n",
    "dataset = \"mistral_mmlu\"\n",
    "variant_groups = {\n",
    "    # \"original\": [f\"mistraljson_variant\"],\n",
    "    # \"blank_space\": [f\"mistraljson_variant\", f\"{dataset}_blank_space_1\", f\"{dataset}_blank_space_2\"],\n",
    "    # \"shuffled_option\": [f\"mistraljson_variant\", f\"{dataset}_shuffled_option_1\", f\"{dataset}_shuffled_option_2\"],\n",
    "    # \"typo\": [f\"mistraljson_variant\", f\"{dataset}_typo_1\", f\"{dataset}_typo_2\"],\n",
    "    f\"{model_name}\": [\"mistraljson_oneshot\"]\n",
    "}\n",
    "\n",
    "# Process each variant group\n",
    "for group_name, subdirectories in variant_groups.items():\n",
    "    print(f\"Processing variant group: {group_name}\")\n",
    "    \n",
    "    # Initialize matrix to accumulate values across setups\n",
    "    n_methods = len(methods)\n",
    "    avg_matrix_per_setup = np.zeros((n_methods, n_methods, 5))  # 5 setups, n_methods x n_methods for each setup\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        flags_per_method = defaultdict(dict)  # Store flags per method for each subdirectory\n",
    "\n",
    "        # Process all methods for the current subdirectory\n",
    "        for method in methods:\n",
    "            flags_per_method[method] = process_json_files_for_method(directory, subdirectory, model_name, method)\n",
    "\n",
    "        # Compute pairwise Harmonic IOU for all setups\n",
    "        for setup_index in range(5):  # Setup indices from 0 to 4\n",
    "            for i in range(n_methods):\n",
    "                method_i = methods[i]\n",
    "                flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "                \n",
    "                for j in range(i, n_methods):\n",
    "                    method_j = methods[j]\n",
    "                    flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "                    \n",
    "                    # Calculate the Harmonic IOU\n",
    "                    if i == j:\n",
    "                        harmonic_iou = 1  # Diagonal value\n",
    "                    else:\n",
    "                        harmonic_iou = calculate_harmonic_iou(flags_i, flags_j)\n",
    "\n",
    "                    # Accumulate values\n",
    "                    avg_matrix_per_setup[i, j, setup_index] += harmonic_iou\n",
    "                    if i != j:\n",
    "                        avg_matrix_per_setup[j, i, setup_index] += harmonic_iou  # Symmetric matrix\n",
    "\n",
    "    # Average across subdirectories\n",
    "    avg_matrix_per_setup /= len(subdirectories)\n",
    "\n",
    "    # Plot the heatmap for each setup\n",
    "    for setup_index in range(1):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if \"llama\" in dataset:\n",
    "        #sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" LLaMA3, MMLU - Harmonic IOU Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"h_llama_mmlu_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" LLaMA3, Hellaswag - Harmonic IOU Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"h_llama_hellaswag_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        else:\n",
    "            if \"mmlu\" in dataset:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" Mistral, MMLU - Harmonic IOU Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"h_mistral_mmlu_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "                print(\"i\")\n",
    "            else:\n",
    "                sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=display_methods, yticklabels=display_methods, cmap='Blues', fmt=\".2f\")\n",
    "                plt.title(f\" Mistral, Hellaswag - Harmonic IOU Heatmap (One-shot)\")\n",
    "                plt.savefig(f\"h_mistral_hellaswag_shot.pdf\", format='pdf', bbox_inches='tight')\n",
    "        # sns.heatmap(avg_matrix_per_setup[:, :, setup_index], annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues', fmt=\".2f\")\n",
    "        # plt.title(f\"Harmonic IOU Heatmap ({group_name.capitalize()})\")\n",
    "        # plt.xlabel(\"Methods\")\n",
    "        # plt.ylabel(\"Methods\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(f\"{model_name}_{group_name}_harmonic_iou.pdf\", format='pdf', bbox_inches='tight')\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755b8a6",
   "metadata": {},
   "source": [
    "## Average Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize variables to hold data\n",
    "flags_per_method = defaultdict(dict)  # Store flags (abstain or answer) for each method by setup index\n",
    "\n",
    "# Define function to calculate the ratio of common flags (both abstain or both answer)\n",
    "def calculate_common_flag_ratio(flags1, flags2):\n",
    "    if not flags1 or not flags2:  # If one of the flag sets is empty, return 0\n",
    "        return 0\n",
    "    \n",
    "    total_questions = len(flags1)\n",
    "    same_flags_count = sum(1 for q in flags1 if flags1[q] == flags2.get(q, None))\n",
    "\n",
    "    return same_flags_count / total_questions if total_questions > 0 else 0\n",
    "\n",
    "# Define function to process JSON files for a specific method and model\n",
    "def process_json_files_for_method(directory, model_name, method):\n",
    "    flags = defaultdict(dict)  # Flags (abstain/answer) per question for a specific method\n",
    "\n",
    "    # Process JSON files\n",
    "    for filename in os.listdir(directory):\n",
    "        if model_name in filename and method in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            eval_index = int(filename.split('_')[-1].split('.')[0])  # Extract setup index (0-4)\n",
    "            process_json_file(file_path, eval_index, flags)\n",
    "            print(file_path)\n",
    "\n",
    "    # Store the flags for the current method\n",
    "    flags_per_method[method] = flags\n",
    "\n",
    "# Function to process each JSON file and extract relevant flags\n",
    "def process_json_file(file_path, eval_index, flags):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for entry in data:\n",
    "            question_idx = entry['question_idx']\n",
    "            decision = entry['decision']  # Either 'abstain' or 'answer'\n",
    "\n",
    "            # Collect flags for all questions (both abstain and answer)\n",
    "            flags[eval_index][question_idx] = decision\n",
    "\n",
    "# List of methods to process\n",
    "methods = ['calibration', 'embedding', 'nota', 'moreinfo', 'token', 'reflect']\n",
    "model_name = \"llama3\"\n",
    "\n",
    "directory = \"Probing_Uncertainy\"\n",
    "# Loop over each method and process corresponding JSON files for the Mistral model\n",
    "for method in methods:\n",
    "    process_json_files_for_method(directory, model_name, method)\n",
    "\n",
    "# Initialize matrices to accumulate sums of common flag ratios\n",
    "n_methods = len(methods)\n",
    "all_setups_matrix = np.zeros((n_methods, n_methods))  # For setups 0 to 4\n",
    "setup_1_to_4_matrix = np.zeros((n_methods, n_methods))  # For setups 1 to 4\n",
    "count_1_to_4 = 0  # To count the number of setups (1 to 4)\n",
    "\n",
    "# Loop over each setup index (0 to 4) and accumulate common flag ratios\n",
    "for setup_index in range(5,9):  # Setup indices from 0 to 4\n",
    "    # Initialize matrix to hold common flag ratios for this setup\n",
    "    common_flag_matrix = np.zeros((n_methods, n_methods))\n",
    "\n",
    "    # Calculate pairwise common flag ratios for the same setup across methods\n",
    "    for i in range(n_methods):\n",
    "        method_i = methods[i]\n",
    "        flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "        \n",
    "        for j in range(i, n_methods):\n",
    "            method_j = methods[j]\n",
    "            flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "            \n",
    "            # If comparing the method with itself, set the diagonal value to 1\n",
    "            if i == j:\n",
    "                common_flag_matrix[i, j] = 1\n",
    "            else:\n",
    "                # Calculate the common flag ratio for different methods\n",
    "                common_flag_ratio = calculate_common_flag_ratio(flags_i, flags_j)\n",
    "                common_flag_matrix[i, j] = common_flag_ratio\n",
    "                common_flag_matrix[j, i] = common_flag_ratio  # Symmetric matrix\n",
    "\n",
    "    # Accumulate the matrix for all setups\n",
    "    all_setups_matrix += common_flag_matrix\n",
    "\n",
    "    # If setup index is 1 to 4, accumulate for setups 1 to 4\n",
    "    if setup_index <= 8:\n",
    "        setup_1_to_4_matrix += common_flag_matrix\n",
    "        count_1_to_4 += 1\n",
    "\n",
    "# Average the matrix for all setups (0 to 4)\n",
    "all_setups_average = all_setups_matrix / 5  # We have 5 setups (0 to 4)\n",
    "\n",
    "# Average the matrix for setups 1 to 4\n",
    "setup_1_to_4_average = setup_1_to_4_matrix / count_1_to_4  # We accumulated setups 1 to 4\n",
    "\n",
    "if model_name =='mistral_mmlu':\n",
    "    title = 'Mistral'\n",
    "else:\n",
    "    title = 'Llama'\n",
    "# Plotting the heatmap for the average common flag ratio (all setups)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(all_setups_average, annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues')\n",
    "plt.title(f'{title} Average Common Flag Ratio Heatmap (All setups)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the heatmap for the average common flag ratio (setups 1 to 4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(setup_1_to_4_average, annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues')\n",
    "plt.title(f'LLaMa3, Hellaswag - Common Flag Ratio Heatmap (Oneshot)')\n",
    "plt.savefig(f\"{model_name}_{group_name}_setup_{setup_index}_harmonic_iou.pdf\", format='pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each setup index (0 to 4) and plot individual heatmaps\n",
    "for setup_index in range(5,9):  # Setup indices from 0 to 4\n",
    "    # Initialize matrix to hold common flag ratios for this setup\n",
    "    common_flag_matrix = np.zeros((n_methods, n_methods))\n",
    "\n",
    "    # Calculate pairwise common flag ratios for the same setup across methods\n",
    "    for i in range(n_methods):\n",
    "        method_i = methods[i]\n",
    "        flags_i = flags_per_method[method_i].get(setup_index, {})  # Get flags for method_i\n",
    "        \n",
    "        for j in range(i, n_methods):\n",
    "            method_j = methods[j]\n",
    "            flags_j = flags_per_method[method_j].get(setup_index, {})  # Get flags for method_j\n",
    "            \n",
    "            # If comparing the method with itself, set the diagonal value to 1\n",
    "            if i == j:\n",
    "                common_flag_matrix[i, j] = 1\n",
    "            else:\n",
    "                # Calculate the common flag ratio for different methods\n",
    "                common_flag_ratio = calculate_common_flag_ratio(flags_i, flags_j)\n",
    "                common_flag_matrix[i, j] = common_flag_ratio\n",
    "                common_flag_matrix[j, i] = common_flag_ratio  # Symmetric matrix\n",
    "\n",
    "    # Plotting the heatmap for the current setup\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(common_flag_matrix, annot=True, xticklabels=methods, yticklabels=methods, cmap='Blues')\n",
    "    plt.title(f'{title} Common Flag Ratio Heatmap (Setup {setup_index})')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
